project_id  = "your-project-id"
region      = "europe-west1"
environment = "prod"

domain_name        = "example.com"
additional_domains = ["www.example.com"]

llm_service = {
  name          = "ollama-llm"
  image         = "europe-west1-docker.pkg.dev/your-project/backend/zinovia-ollama:latest"
  model         = "llama3.2"
  cpu           = "4"
  memory        = "16Gi"
  min_instances = 0
  max_instances = 1
  concurrency   = 1
  timeout_seconds = 600
  ingress         = "INGRESS_TRAFFIC_INTERNAL_ONLY"
  env_vars = {
    OLLAMA_KEEP_ALIVE = "5m"
  }
}

frontend_services = [
  {
    name          = "web-frontend"
    image         = "europe-west1-docker.pkg.dev/your-project/zinovia-frontend/web-frontend:latest"
    min_instances = 1
    max_instances = 20
    concurrency   = 80
    use_llm_service = true
    env_vars = {
      APP_ENV = "production"
      LLM_API_PATH   = "/api/chat"
      LLM_MODEL_NAME = "llama3.2"
    }
    secrets = [
      {
        env_name = "SOME_API_KEY"
        secret   = "projects/your-project/secrets/some-api-key"
        version  = "latest"
      }
    ]
  }
]

monitoring_notification_channels = [
  "projects/your-project/notificationChannels/0000000000000000000"
]

